{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load data</a></span></li><li><span><a href=\"#Get-the-features-and-target\" data-toc-modified-id=\"Get-the-features-and-target-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get the features and target</a></span></li><li><span><a href=\"#The-brute-force-approach-for-model-evaluation\" data-toc-modified-id=\"The-brute-force-approach-for-model-evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The brute-force approach for model evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Divide-the-data-into-training-and-testing\" data-toc-modified-id=\"Divide-the-data-into-training-and-testing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Divide the data into training and testing</a></span></li><li><span><a href=\"#Standardization\" data-toc-modified-id=\"Standardization-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Standardization</a></span></li><li><span><a href=\"#Training-and-testing\" data-toc-modified-id=\"Training-and-testing-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Training and testing</a></span></li></ul></li><li><span><a href=\"#A-better-approach\" data-toc-modified-id=\"A-better-approach-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>A better approach</a></span><ul class=\"toc-item\"><li><span><a href=\"#Divide-the-data-into-$k$-folds\" data-toc-modified-id=\"Divide-the-data-into-$k$-folds-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Divide the data into $k$ folds</a></span></li><li><span><a href=\"#Standardization,-training,-and-testing\" data-toc-modified-id=\"Standardization,-training,-and-testing-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Standardization, training, and testing</a></span></li></ul></li><li><span><a href=\"#The-best-practice:-Pipeline-+-cross_val_score-+-KFold\" data-toc-modified-id=\"The-best-practice:-Pipeline-+-cross_val_score-+-KFold-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>The best practice: Pipeline + cross_val_score + KFold</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-the-pipeline\" data-toc-modified-id=\"Create-the-pipeline-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Create the pipeline</a></span></li><li><span><a href=\"#Calculate-the-average-accuracy-using-the-pipeline,-cross_val_score,-and-KFold\" data-toc-modified-id=\"Calculate-the-average-accuracy-using-the-pipeline,-cross_val_score,-and-KFold-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Calculate the average accuracy using the pipeline, cross_val_score, and KFold</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Machine Learning I (DATS 6202 - O10), Spring 2019\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Exercise 2 (Solution)\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Yuxiao Huang\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Apply Linear Regression on the Housing dataset\n",
    "- Specifically, you should use the following linear model\n",
    "$$\n",
    "MEDV = w_0 + w_1 x_1 + w_2 x_2 +, \\ldots, + w_n x_n.\n",
    "$$\n",
    "- Here:\n",
    "    - $MEDV$ is the target, which is the median value of owner-occupied homes in \\$1000s\n",
    "    - $x_1, x_2, \\ldots, x_n$ are the features in the input dataset\n",
    "    - $w_0, w_1, \\ldots, w_n$ are the parameters\n",
    "- The goal of this exercise is to\n",
    "    - walk through the steps in data preprocessing, training, testing, and model evaluation, by dividing the data using a single split and by using cross validation\n",
    "    - introduce the best practice for data preprocessing, training, testing, and model evaluation\n",
    "- Complete the missing parts indicated by # Implement me\n",
    "- Particularly, the code should\n",
    "    - be bug-free (while the output produced by your solution being the same as the provided output does not necessarily mean your code is bug-free, it is very likely that there is a bug in your code when the two kinds of output are different)\n",
    "    - be commented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rasbt/'\n",
    "                 'python-machine-learning-book-2nd-edition'\n",
    "                 '/master/code/ch10/housing.data.txt',\n",
    "                 header=None,\n",
    "                 sep='\\s+')\n",
    "\n",
    "df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', \n",
    "              'NOX', 'RM', 'AGE', 'DIS', 'RAD', \n",
    "              'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement me\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['MEDV'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The brute-force approach for model evaluation\n",
    "Here we use a single split to divide the data into training and testing. The accuracy of the model (learned from the training data) is then obtained from the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the data into training and testing (with test_size=0.3 and random_state = 0)\n",
    "# Implement me\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features and target\n",
    "# Implement me\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "y_train = ss.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "y_test = ss.transform(y_test.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the model:  0.673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Declare the linear regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "# Implement me\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print('The score of the model: ', round(lr.score(X_test, y_test), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A better approach\n",
    "Here we will use cross validation to divide the data into $k$ folds. Unlike in the previous case (when using a single split) where there is only one round of training and testing, here there are $k$ rounds. For round $i$, fold $i$ is used for testing and the remaining folds are used for training. This results in a list of $k$ accuracy across the $k$ rounds. We can then report the mean and standard deviation (std) of this list. The mean measures the average accuracy of the model. Generally the higher the mean the more accurate the model. The std, on the other hand, measures the stability of the model. Generally the lower the std the more stable the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into $k$ folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# StratifiedKFold (with n_splits=10 and random_state=0)\n",
    "# Implement me\n",
    "kf = KFold(n_splits=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization, training, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of the model:  0.203\n",
      "The std score of the accuracy:  0.595\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "accs = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    # Get the training and testing data using train_idx and test_idx\n",
    "    # Implement me\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # Standardize the features and target\n",
    "    # Implement me\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_test = ss.transform(X_test)\n",
    "    y_train = ss.fit_transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "    y_test = ss.transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    # Implement me\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Add the accuracy\n",
    "    accs.append(lr.score(X_test, y_test))\n",
    "    \n",
    "print('The average score of the model: ', round(np.mean(accs), 3))\n",
    "print('The std score of the accuracy: ', round(np.std(accs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The best practice: Pipeline + cross_val_score + KFold\n",
    "While the approach based on cross validation is accurate, it is not the best practice. Here we show a much simpler approach using the pipeline, cross_val_score, and KFold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline\n",
    "As the name suggested, the [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a chain where the steps in data preprocessing are connected. In reality, the pipeline is a list of tuples (pairs), where the second item in the tuple is a transformer and the first item the name of the transformer. The pipeline will, as you will see in this exercise and later ones, greatly simplify data preprocessing, training and testing, hyperparameter tuning, and model selection.\n",
    "\n",
    "There are two things you should know about the pipeline. First, the pipeline must end with an estimator (a regressior or classifier). Second, the order of the transformers in the pipeline matters. For any two transformers A and B, the transformation provided by A will be performed prior to that provided by B, if A is before B in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# The pipeline with StandardScaler and LinearRegression\n",
    "pipe_lr = Pipeline([('StandardScaler', StandardScaler()), ('LinearRegression', LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the average accuracy using the pipeline, cross_val_score, and KFold\n",
    "The [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function is a one-stop-shop that wraps what we did in sec. 5. That is, dividing the data into $k$ folds, and in each of the $k$ rounds, first standardizing the data then training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score of the model:  0.203\n",
      "The std score of the accuracy:  0.595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Get the list of accuracy obtained by cross_val_score\n",
    "accs = cross_val_score(pipe_lr,\n",
    "                       X,\n",
    "                       y,\n",
    "                       cv=KFold(n_splits=10, random_state=0))\n",
    "\n",
    "print('The average score of the model: ', round(accs.mean(), 3))\n",
    "print('The std score of the accuracy: ', round(accs.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
